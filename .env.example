# codespy Configuration
# Copy this file to .env and fill in your values
#
# NOTE: For advanced configuration (per-signature settings, model overrides),
# use codespy.yaml instead. See codespy.yaml for full options.
# Priority: Environment Variables > YAML Config > Defaults

# =============================================================================
# Git Platform Tokens
# =============================================================================

# GitHub Token
# Create a token at: https://github.com/settings/tokens
# Required scopes: repo (for private repos) or public_repo (for public repos only)
# Auto-discovered from: gh CLI, git credential helper, ~/.netrc, $GITHUB_TOKEN, $GH_TOKEN
GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Disable GitHub token auto-discovery (default: true)
# GITHUB_AUTO_DISCOVER_TOKEN=false

# GitLab Token (for GitLab MRs)
# Create a token at: https://gitlab.com/-/profile/personal_access_tokens
# Required scopes: api (for full access) or read_api + write_repository
# Auto-discovered from: glab CLI, git credential helper, ~/.netrc, ~/.python-gitlab.cfg
# GITLAB_TOKEN=glpat-xxxxxxxxxxxxxxxxxxxx
# GITLAB_URL=https://gitlab.com
# Disable GitLab token auto-discovery (default: true)
# GITLAB_AUTO_DISCOVER_TOKEN=false

# =============================================================================
# LLM Configuration (choose one provider)
# =============================================================================

# Default model identifier (LiteLLM format)
# Examples:
#   - gpt-5                                          (OpenAI)
#   - gpt-4-turbo                                     (OpenAI)
#   - bedrock/anthropic.claude-opus-4-5-20251101-v1:0 (AWS Bedrock)
#   - bedrock/anthropic.claude-sonnet-4-5-20250929-v1:0 (AWS Bedrock)
#   - bedrock/anthropic.claude-haiku-4-5-20260112-v1:0  (AWS Bedrock)
#   - claude-4-5-opus-20251101                        (Anthropic direct)
#   - claude-4-5-sonnet-20251018                      (Anthropic direct)
#   - gemini/gemini-2.5-pro                           (Google Gemini)
#   - gemini/gemini-2.5-flash                         (Google Gemini)
#   - ollama/llama-4-70b                              (Local Ollama)
DEFAULT_MODEL=gpt-5

# -----------------------------------------------------------------------------
# Option 1: OpenAI
# -----------------------------------------------------------------------------
# Auto-discovered from: $OPENAI_API_KEY, ~/.config/openai/, ~/.openai/
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Option 2: AWS Bedrock
# -----------------------------------------------------------------------------
# Auto-discovered from: ~/.aws/credentials, AWS CLI, $AWS_ACCESS_KEY_ID
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
# AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
# AWS_PROFILE=default

# -----------------------------------------------------------------------------
# Option 3: Anthropic (direct)
# -----------------------------------------------------------------------------
# Auto-discovered from: $ANTHROPIC_API_KEY, ~/.config/anthropic/, ~/.anthropic/
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Option 4: Google Gemini
# -----------------------------------------------------------------------------
# Auto-discovered from: $GEMINI_API_KEY, $GOOGLE_API_KEY, gcloud ADC
# GEMINI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Option 5: Azure OpenAI
# -----------------------------------------------------------------------------
# AZURE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-15-preview

# =============================================================================
# Auto-Discovery Settings
# =============================================================================
# Disable auto-discovery for specific providers (all default to true)
# AUTO_DISCOVER_OPENAI=false
# AUTO_DISCOVER_ANTHROPIC=false
# AUTO_DISCOVER_GEMINI=false
# AUTO_DISCOVER_AWS=false

# =============================================================================
# Default Settings
# =============================================================================
# These apply to all signatures unless overridden per-signature

# DEFAULT_MODEL=gpt-5
# Extraction model for TwoStepAdapter (extracts structured fields from free-form LLM responses)
# Uses a smaller/faster model for cost-effective field extraction
# EXTRACTION_MODEL=claude-haiku-4-5-20251001
# DEFAULT_MAX_ITERS=3
# DEFAULT_MAX_CONTEXT_SIZE=50000
# Limits LLM reasoning verbosity (helps prevent JSONAdapter failures)
# DEFAULT_MAX_REASONING_TOKENS=1024
# Lower = more deterministic JSON output
# DEFAULT_TEMPERATURE=0.1

# Global LLM reliability settings
# Number of retries for LLM API calls
# LLM_RETRIES=3
# Timeout in seconds for LLM calls
# LLM_TIMEOUT=120

# Enable provider-side prompt caching (reduces latency and costs)
# ENABLE_PROMPT_CACHING=true

# =============================================================================
# Output Settings
# =============================================================================

# Output format: markdown or json (default: markdown)
# OUTPUT_FORMAT=markdown

# Enable stdout output (default: true)
# OUTPUT_STDOUT=true

# Post review to Git platform (GitHub PR or GitLab MR) (default: false)
# OUTPUT_GIT=false

# Cache directory for cloned repositories (default: ~/.cache/codespy)
# CACHE_DIR=/path/to/cache

# =============================================================================
# Per-Signature Configuration (override via env vars)
# =============================================================================
# Use underscore (_) between signature name and setting
# Format: SIGNATURE_NAME_SETTING=value
#
# Available signatures:
#   - CODE_SECURITY      (security vulnerability detection)
#   - SUPPLY_CHAIN       (supply chain security analysis)
#   - BUG_DETECTION      (bug and logic error detection)
#   - DOC_REVIEW         (documentation review)
#   - DOMAIN_ANALYSIS    (domain expert analysis)
#   - SCOPE_IDENTIFICATION (code scope detection)
#   - DEDUPLICATION      (issue deduplication)
#   - SUMMARIZATION      (PR summary generation)
#
# Available settings per signature:
#   - ENABLED (true/false)
#   - MAX_ITERS (integer)
#   - MODEL (LiteLLM model string)
#   - MAX_CONTEXT_SIZE (integer)
#   - MAX_REASONING_TOKENS (integer) - Limits reasoning verbosity
#   - TEMPERATURE (float) - Lower = more deterministic output

# Examples:
# CODE_SECURITY_ENABLED=true
# CODE_SECURITY_MAX_ITERS=10
# CODE_SECURITY_MODEL=claude-sonnet-4-5-20250929
# CODE_SECURITY_MAX_REASONING_TOKENS=512
# CODE_SECURITY_TEMPERATURE=0.1

# SUPPLY_CHAIN_ENABLED=true

# BUG_DETECTION_ENABLED=true
# BUG_DETECTION_MAX_ITERS=5

# DOC_REVIEW_ENABLED=true
# DOC_REVIEW_MODEL=claude-haiku-4-5-20251001

# DOMAIN_ANALYSIS_ENABLED=false
# DOMAIN_ANALYSIS_MAX_ITERS=6

# SCOPE_IDENTIFICATION_ENABLED=true
# SCOPE_IDENTIFICATION_MAX_ITERS=10
# More reasoning for complex scope detection
# SCOPE_IDENTIFICATION_MAX_REASONING_TOKENS=1024

# DEDUPLICATION_ENABLED=true
# DEDUPLICATION_MODEL=claude-haiku-4-5-20251001

# SUMMARIZATION_ENABLED=true
# SUMMARIZATION_MODEL=claude-haiku-4-5-20251001
