# codespy Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# REQUIRED: GitHub Token
# =============================================================================
# Create a token at: https://github.com/settings/tokens
# Required scopes: repo (for private repos) or public_repo (for public repos only)
GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# =============================================================================
# LLM Configuration (choose one provider)
# =============================================================================

# Model identifier (LiteLLM format)
# Examples:
#   - gpt-4o                              (OpenAI)
#   - gpt-4-turbo                         (OpenAI)
#   - bedrock/anthropic.claude-opus-4-5-20251101-v1:0  (AWS Bedrock)
#   - bedrock/anthropic.claude-sonnet-4-5-20250929-v1:0:1m  (AWS Bedrock)
#   - bedrock/anthropic.claude-haiku-4-5-20260112-v1:0   (AWS Bedrock)
#   - claude-4-5-opus-20251101            (Anthropic direct)
#   - claude-4-5-sonnet-20251018          (Anthropic direct)
#   - claude-4-5-haiku-20260110           (Anthropic direct)
#   - gemini-3-pro-001                    (Google Vertex/AI Studio)
#   - gemini-3-flash-001                  (Google Vertex/AI Studio)
#   - ollama/llama-4-70b                  (Local Ollama)
#   - ollama/mistral-large-2025-v2        (Local Ollama)
# Use cross-region inference profile for Opus 4.5
LITELLM_MODEL=bedrock/us.anthropic.claude-opus-4-5-20251101-v1:0

# -----------------------------------------------------------------------------
# Option 1: OpenAI
# -----------------------------------------------------------------------------
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Option 2: AWS Bedrock
# -----------------------------------------------------------------------------
# If using Bedrock, set the region and optionally credentials
# (credentials are optional if using IAM roles or ~/.aws/credentials)
AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
# AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

# -----------------------------------------------------------------------------
# Option 3: Anthropic (direct)
# -----------------------------------------------------------------------------
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# =============================================================================
# Optional Settings
# =============================================================================

# Maximum context size in characters for LLM input (default: 50000)
# MAX_CONTEXT_SIZE=50000

# Default output format: markdown or json (default: markdown)
# OUTPUT_FORMAT=markdown

# Cache directory for cloned repositories (default: ~/.cache/codespy)
# CACHE_DIR=/path/to/cache