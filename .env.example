# codespy Configuration
# Copy this file to .env and fill in your values
#
# NOTE: For advanced configuration (per-module settings, model overrides),
# use codespy.yaml instead. See codespy.example.yaml for full options.
# Priority: Environment Variables > YAML Config > Defaults

# =============================================================================
# REQUIRED: GitHub Token
# =============================================================================
# Create a token at: https://github.com/settings/tokens
# Required scopes: repo (for private repos) or public_repo (for public repos only)
# Auto-discovered from: gh CLI, git credential helper, ~/.netrc
GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# =============================================================================
# LLM Configuration (choose one provider)
# =============================================================================

# Default model identifier (LiteLLM format)
# Examples:
#   - gpt-4o                                          (OpenAI)
#   - gpt-4-turbo                                     (OpenAI)
#   - bedrock/anthropic.claude-opus-4-5-20251101-v1:0 (AWS Bedrock)
#   - bedrock/anthropic.claude-sonnet-4-5-20250929-v1:0 (AWS Bedrock)
#   - bedrock/anthropic.claude-haiku-4-5-20260112-v1:0  (AWS Bedrock)
#   - claude-4-5-opus-20251101                        (Anthropic direct)
#   - claude-4-5-sonnet-20251018                      (Anthropic direct)
#   - gemini/gemini-1.5-pro                           (Google Gemini)
#   - gemini/gemini-1.5-flash                         (Google Gemini)
#   - ollama/llama-4-70b                              (Local Ollama)
DEFAULT_MODEL=gpt-4o

# -----------------------------------------------------------------------------
# Option 1: OpenAI
# -----------------------------------------------------------------------------
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Option 2: AWS Bedrock
# -----------------------------------------------------------------------------
# (credentials optional if using IAM roles or ~/.aws/credentials)
# AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
# AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
# AWS_PROFILE=default

# -----------------------------------------------------------------------------
# Option 3: Anthropic (direct)
# -----------------------------------------------------------------------------
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Option 4: Google Gemini
# -----------------------------------------------------------------------------
# GEMINI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Option 5: Azure OpenAI
# -----------------------------------------------------------------------------
# AZURE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-15-preview

# =============================================================================
# Default Settings
# =============================================================================
# These apply to all modules unless overridden per-module

# DEFAULT_MODEL=gpt-4o
# DEFAULT_MAX_ITERS=10
# DEFAULT_MAX_CONTEXT_SIZE=50000

# =============================================================================
# Per-Module Configuration (override via env vars)
# =============================================================================
# Use double underscore (__) for nested keys
# Format: MODULE_NAME__SETTING=value

# Per-module overrides (enabled, max_iters, model)
# SECURITY_AUDITOR__ENABLED=true
# SECURITY_AUDITOR__MAX_ITERS=10
# BUG_DETECTOR__ENABLED=true
# DOC_REVIEWER__ENABLED=true
# DOC_REVIEWER__MAX_ITERS=15
# DOMAIN_EXPERT__ENABLED=true
# DOMAIN_EXPERT__MAX_ITERS=30
# SCOPE_IDENTIFIER__MAX_ITERS=20
# DEDUPLICATOR__MODEL=gpt-3.5-turbo
# SUMMARIZER__MODEL=gpt-3.5-turbo

# =============================================================================
# Optional Settings
# =============================================================================

# Default output format: markdown or json (default: markdown)
# OUTPUT_FORMAT=markdown

# Cache directory for cloned repositories (default: ~/.cache/codespy)
# CACHE_DIR=/path/to/cache

# Include vendor/dependency files in review (default: false)
# INCLUDE_VENDOR=false