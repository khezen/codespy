name: 'CodeSpy Code Review'
description: 'Automated code review powered by AI. Detects bugs, security vulnerabilities, and documentation issues in your PRs.'
author: 'khezen'
branding:
  icon: 'eye'
  color: 'purple'

inputs:
  github-token:
    description: 'GitHub token for PR access and posting comments. Uses GITHUB_TOKEN by default.'
    required: false
    default: ${{ github.token }}
  
  model:
    description: 'Default LLM model to use (e.g., anthropic/claude-sonnet-4-5-20250929, openai/gpt-5, bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0)'
    required: true
  
  extraction-model:
    description: 'Model for field extraction (smaller model recommended)'
    required: false
    default: 'anthropic/claude-haiku-4-5-20251001'
  
  pr-url:
    description: 'URL of the pull request to review. Defaults to the current PR context.'
    required: false
    default: ''
  
  post-comment:
    description: 'Post review as a Git platform PR/MR comment'
    required: false
    default: 'true'
  
  output-format:
    description: 'Output format: markdown or json'
    required: false
    default: 'markdown'
  
  # LLM Provider API Keys
  anthropic-api-key:
    description: 'Anthropic API key (for Claude models)'
    required: false
  
  openai-api-key:
    description: 'OpenAI API key (for GPT models)'
    required: false
  
  gemini-api-key:
    description: 'Google Gemini API key'
    required: false
  
  # AWS Bedrock credentials
  aws-access-key-id:
    description: 'AWS Access Key ID (for Bedrock models)'
    required: false
  
  aws-secret-access-key:
    description: 'AWS Secret Access Key (for Bedrock models)'
    required: false
  
  aws-region:
    description: 'AWS Region (for Bedrock models)'
    required: false
    default: 'us-east-1'
  
  # Global defaults
  default-max-iters:
    description: 'Default maximum iterations for all signatures'
    required: false
    default: '3'
  
  default-max-context-size:
    description: 'Default maximum context size for all signatures'
    required: false
    default: '50000'
  
  default-max-reasoning-tokens:
    description: 'Default maximum reasoning tokens for all signatures'
    required: false
    default: '8000'
  
  default-temperature:
    description: 'Default temperature for LLM calls (0.0-1.0)'
    required: false
    default: '0.1'
  
  llm-retries:
    description: 'Number of retries for LLM API calls'
    required: false
    default: '3'
  
  llm-timeout:
    description: 'Timeout in seconds for LLM calls'
    required: false
    default: '120'
  
  enable-prompt-caching:
    description: 'Enable provider-side prompt caching (reduces latency and costs)'
    required: false
    default: 'true'
  
  # ==========================================
  # SIGNATURE: scope
  # ==========================================
  scope-enabled:
    description: 'Enable scope identification signature'
    required: false
    default: 'true'
  
  scope-model:
    description: 'Model for scope identification (empty = use default)'
    required: false
  
  scope-max-iters:
    description: 'Max iterations for scope identification'
    required: false
  
  scope-max-context-size:
    description: 'Max context size for scope identification'
    required: false
  
  scope-max-reasoning-tokens:
    description: 'Max reasoning tokens for scope identification'
    required: false
  
  scope-temperature:
    description: 'Temperature for scope identification'
    required: false
  
  # ==========================================
  # SIGNATURE: defect
  # ==========================================
  defect-enabled:
    description: 'Enable defect detection signature (bugs, removed defensive code, security vulnerabilities)'
    required: false
    default: 'true'
  
  defect-model:
    description: 'Model for defect detection (empty = use default)'
    required: false
  
  defect-max-iters:
    description: 'Max iterations for defect detection'
    required: false
  
  defect-max-context-size:
    description: 'Max context size for defect detection'
    required: false
  
  defect-max-reasoning-tokens:
    description: 'Max reasoning tokens for defect detection'
    required: false
  
  defect-temperature:
    description: 'Temperature for defect detection'
    required: false
  
  # ==========================================
  # SIGNATURE: doc
  # ==========================================
  doc-enabled:
    description: 'Enable documentation review signature (stale/wrong documentation)'
    required: false
    default: 'true'
  
  doc-model:
    description: 'Model for doc review (empty = use default)'
    required: false
  
  doc-max-iters:
    description: 'Max iterations for doc review'
    required: false
  
  doc-max-context-size:
    description: 'Max context size for doc review'
    required: false
  
  doc-max-reasoning-tokens:
    description: 'Max reasoning tokens for doc review'
    required: false
  
  doc-temperature:
    description: 'Temperature for doc review'
    required: false
  
  # ==========================================
  # SIGNATURE: smell
  # ==========================================
  smell-enabled:
    description: 'Enable code smell detection signature (naming, complexity, YAGNI)'
    required: false
    default: 'true'
  
  smell-model:
    description: 'Model for smell detection (empty = use default)'
    required: false
  
  smell-max-iters:
    description: 'Max iterations for smell detection'
    required: false
  
  smell-max-context-size:
    description: 'Max context size for smell detection'
    required: false
  
  smell-max-reasoning-tokens:
    description: 'Max reasoning tokens for smell detection'
    required: false
  
  smell-temperature:
    description: 'Temperature for smell detection'
    required: false
  
  # ==========================================
  # SIGNATURE: supply_chain
  # ==========================================
  supply-chain-enabled:
    description: 'Enable supply chain security signature'
    required: false
    default: 'true'
  
  supply-chain-model:
    description: 'Model for supply chain security (empty = use default)'
    required: false
  
  supply-chain-max-iters:
    description: 'Max iterations for supply chain security'
    required: false
  
  supply-chain-max-context-size:
    description: 'Max context size for supply chain security'
    required: false
  
  supply-chain-max-reasoning-tokens:
    description: 'Max reasoning tokens for supply chain security'
    required: false
  
  supply-chain-temperature:
    description: 'Temperature for supply chain security'
    required: false
  
  supply-chain-scan-unchanged:
    description: 'Scan all artifacts/manifests even if not modified in the PR (default: only scan changed)'
    required: false
    default: 'false'
  
  # ==========================================
  # SIGNATURE: deduplication
  # ==========================================
  deduplication-enabled:
    description: 'Enable issue deduplication signature'
    required: false
    default: 'true'
  
  deduplication-model:
    description: 'Model for deduplication (empty = use default)'
    required: false
  
  deduplication-max-iters:
    description: 'Max iterations for deduplication'
    required: false
  
  deduplication-max-context-size:
    description: 'Max context size for deduplication'
    required: false
  
  deduplication-max-reasoning-tokens:
    description: 'Max reasoning tokens for deduplication'
    required: false
  
  deduplication-temperature:
    description: 'Temperature for deduplication'
    required: false
  
  # ==========================================
  # SIGNATURE: summarization
  # ==========================================
  summarization-enabled:
    description: 'Enable PR summarization signature'
    required: false
    default: 'true'
  
  summarization-model:
    description: 'Model for summarization (empty = use default)'
    required: false
  
  summarization-max-iters:
    description: 'Max iterations for summarization'
    required: false
  
  summarization-max-context-size:
    description: 'Max context size for summarization'
    required: false
  
  summarization-max-reasoning-tokens:
    description: 'Max reasoning tokens for summarization'
    required: false
  
  summarization-temperature:
    description: 'Temperature for summarization'
    required: false
  
  excluded-directories:
    description: 'JSON array of directories to exclude from review (e.g., ["vendor", "dist"])'
    required: false
  
  fail-on-critical:
    description: 'Fail the action if critical issues are found'
    required: false
    default: 'false'
  
  codespy-version:
    description: 'Version of codespy Docker image to use'
    required: false
    default: 'latest'

outputs:
  issues-count:
    description: 'Total number of issues found'
    value: ${{ steps.review.outputs.issues-count }}
  critical-count:
    description: 'Number of critical issues found'
    value: ${{ steps.review.outputs.critical-count }}
  high-count:
    description: 'Number of high severity issues found'
    value: ${{ steps.review.outputs.high-count }}
  review-output:
    description: 'The full review output'
    value: ${{ steps.review.outputs.review-output }}

runs:
  using: 'composite'
  steps:
    - name: Determine PR URL
      id: pr-url
      shell: bash
      run: |
        if [ -n "${{ inputs.pr-url }}" ]; then
          echo "url=${{ inputs.pr-url }}" >> $GITHUB_OUTPUT
        elif [ "${{ github.event_name }}" = "pull_request" ] || [ "${{ github.event_name }}" = "pull_request_target" ]; then
          echo "url=${{ github.event.pull_request.html_url }}" >> $GITHUB_OUTPUT
        elif [ "${{ github.event_name }}" = "issue_comment" ]; then
          echo "url=${{ github.event.issue.pull_request.html_url }}" >> $GITHUB_OUTPUT
        else
          echo "::error::No PR URL provided and not running in PR context. Please provide pr-url input."
          exit 1
        fi

    - name: Run CodeSpy Review
      id: review
      shell: bash
      env:
        # Core settings
        GITHUB_TOKEN: ${{ inputs.github-token }}
        DEFAULT_MODEL: ${{ inputs.model }}
        EXTRACTION_MODEL: ${{ inputs.extraction-model }}
        OUTPUT_FORMAT: ${{ inputs.output-format }}
        
        # LLM provider credentials
        ANTHROPIC_API_KEY: ${{ inputs.anthropic-api-key }}
        OPENAI_API_KEY: ${{ inputs.openai-api-key }}
        GEMINI_API_KEY: ${{ inputs.gemini-api-key }}
        AWS_ACCESS_KEY_ID: ${{ inputs.aws-access-key-id }}
        AWS_SECRET_ACCESS_KEY: ${{ inputs.aws-secret-access-key }}
        AWS_REGION: ${{ inputs.aws-region }}
        
        # Global defaults
        DEFAULT_MAX_ITERS: ${{ inputs.default-max-iters }}
        DEFAULT_MAX_CONTEXT_SIZE: ${{ inputs.default-max-context-size }}
        DEFAULT_MAX_REASONING_TOKENS: ${{ inputs.default-max-reasoning-tokens }}
        DEFAULT_TEMPERATURE: ${{ inputs.default-temperature }}
        LLM_RETRIES: ${{ inputs.llm-retries }}
        LLM_TIMEOUT: ${{ inputs.llm-timeout }}
        ENABLE_PROMPT_CACHING: ${{ inputs.enable-prompt-caching }}
        
        # Scope identification signature
        SCOPE_ENABLED: ${{ inputs.scope-enabled }}
        SCOPE_MODEL: ${{ inputs.scope-model }}
        SCOPE_MAX_ITERS: ${{ inputs.scope-max-iters }}
        SCOPE_MAX_CONTEXT_SIZE: ${{ inputs.scope-max-context-size }}
        SCOPE_MAX_REASONING_TOKENS: ${{ inputs.scope-max-reasoning-tokens }}
        SCOPE_TEMPERATURE: ${{ inputs.scope-temperature }}
        
        # Defect review signature
        DEFECT_ENABLED: ${{ inputs.defect-enabled }}
        DEFECT_MODEL: ${{ inputs.defect-model }}
        DEFECT_MAX_ITERS: ${{ inputs.defect-max-iters }}
        DEFECT_MAX_CONTEXT_SIZE: ${{ inputs.defect-max-context-size }}
        DEFECT_MAX_REASONING_TOKENS: ${{ inputs.defect-max-reasoning-tokens }}
        DEFECT_TEMPERATURE: ${{ inputs.defect-temperature }}
        
        # Doc review signature
        DOC_ENABLED: ${{ inputs.doc-enabled }}
        DOC_MODEL: ${{ inputs.doc-model }}
        DOC_MAX_ITERS: ${{ inputs.doc-max-iters }}
        DOC_MAX_CONTEXT_SIZE: ${{ inputs.doc-max-context-size }}
        DOC_MAX_REASONING_TOKENS: ${{ inputs.doc-max-reasoning-tokens }}
        DOC_TEMPERATURE: ${{ inputs.doc-temperature }}
        
        # Smell review signature
        SMELL_ENABLED: ${{ inputs.smell-enabled }}
        SMELL_MODEL: ${{ inputs.smell-model }}
        SMELL_MAX_ITERS: ${{ inputs.smell-max-iters }}
        SMELL_MAX_CONTEXT_SIZE: ${{ inputs.smell-max-context-size }}
        SMELL_MAX_REASONING_TOKENS: ${{ inputs.smell-max-reasoning-tokens }}
        SMELL_TEMPERATURE: ${{ inputs.smell-temperature }}
        
        # Supply chain signature
        SUPPLY_CHAIN_ENABLED: ${{ inputs.supply-chain-enabled }}
        SUPPLY_CHAIN_MODEL: ${{ inputs.supply-chain-model }}
        SUPPLY_CHAIN_MAX_ITERS: ${{ inputs.supply-chain-max-iters }}
        SUPPLY_CHAIN_MAX_CONTEXT_SIZE: ${{ inputs.supply-chain-max-context-size }}
        SUPPLY_CHAIN_MAX_REASONING_TOKENS: ${{ inputs.supply-chain-max-reasoning-tokens }}
        SUPPLY_CHAIN_TEMPERATURE: ${{ inputs.supply-chain-temperature }}
        SUPPLY_CHAIN_SCAN_UNCHANGED: ${{ inputs.supply-chain-scan-unchanged }}
        
        # Deduplication signature
        DEDUPLICATION_ENABLED: ${{ inputs.deduplication-enabled }}
        DEDUPLICATION_MODEL: ${{ inputs.deduplication-model }}
        DEDUPLICATION_MAX_ITERS: ${{ inputs.deduplication-max-iters }}
        DEDUPLICATION_MAX_CONTEXT_SIZE: ${{ inputs.deduplication-max-context-size }}
        DEDUPLICATION_MAX_REASONING_TOKENS: ${{ inputs.deduplication-max-reasoning-tokens }}
        DEDUPLICATION_TEMPERATURE: ${{ inputs.deduplication-temperature }}
        
        # Summarization signature
        SUMMARIZATION_ENABLED: ${{ inputs.summarization-enabled }}
        SUMMARIZATION_MODEL: ${{ inputs.summarization-model }}
        SUMMARIZATION_MAX_ITERS: ${{ inputs.summarization-max-iters }}
        SUMMARIZATION_MAX_CONTEXT_SIZE: ${{ inputs.summarization-max-context-size }}
        SUMMARIZATION_MAX_REASONING_TOKENS: ${{ inputs.summarization-max-reasoning-tokens }}
        SUMMARIZATION_TEMPERATURE: ${{ inputs.summarization-temperature }}
        
        # Other settings
        EXCLUDED_DIRECTORIES: ${{ inputs.excluded-directories }}
      run: |
        set -eo pipefail
        
        # Build docker run arguments - add all env vars that are set
        DOCKER_ARGS="--rm"
        
        # Core settings
        DOCKER_ARGS="$DOCKER_ARGS -e GITHUB_TOKEN -e DEFAULT_MODEL -e OUTPUT_FORMAT"
        [ -n "$EXTRACTION_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e EXTRACTION_MODEL"
        
        # Add optional API keys if provided
        [ -n "$ANTHROPIC_API_KEY" ] && DOCKER_ARGS="$DOCKER_ARGS -e ANTHROPIC_API_KEY"
        [ -n "$OPENAI_API_KEY" ] && DOCKER_ARGS="$DOCKER_ARGS -e OPENAI_API_KEY"
        [ -n "$GEMINI_API_KEY" ] && DOCKER_ARGS="$DOCKER_ARGS -e GEMINI_API_KEY"
        [ -n "$AWS_ACCESS_KEY_ID" ] && DOCKER_ARGS="$DOCKER_ARGS -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_REGION"
        
        # Global defaults
        [ -n "$DEFAULT_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFAULT_MAX_ITERS"
        [ -n "$DEFAULT_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFAULT_MAX_CONTEXT_SIZE"
        [ -n "$DEFAULT_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFAULT_MAX_REASONING_TOKENS"
        [ -n "$DEFAULT_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFAULT_TEMPERATURE"
        [ -n "$LLM_RETRIES" ] && DOCKER_ARGS="$DOCKER_ARGS -e LLM_RETRIES"
        [ -n "$LLM_TIMEOUT" ] && DOCKER_ARGS="$DOCKER_ARGS -e LLM_TIMEOUT"
        [ -n "$ENABLE_PROMPT_CACHING" ] && DOCKER_ARGS="$DOCKER_ARGS -e ENABLE_PROMPT_CACHING"
        
        # Scope identification
        [ -n "$SCOPE_ENABLED" ] && DOCKER_ARGS="$DOCKER_ARGS -e SCOPE_ENABLED"
        [ -n "$SCOPE_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e SCOPE_MODEL"
        [ -n "$SCOPE_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SCOPE_MAX_ITERS"
        [ -n "$SCOPE_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SCOPE_MAX_CONTEXT_SIZE"
        [ -n "$SCOPE_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SCOPE_MAX_REASONING_TOKENS"
        [ -n "$SCOPE_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SCOPE_TEMPERATURE"
        
        # Defect review
        [ -n "$DEFECT_ENABLED" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFECT_ENABLED"
        [ -n "$DEFECT_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFECT_MODEL"
        [ -n "$DEFECT_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFECT_MAX_ITERS"
        [ -n "$DEFECT_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFECT_MAX_CONTEXT_SIZE"
        [ -n "$DEFECT_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFECT_MAX_REASONING_TOKENS"
        [ -n "$DEFECT_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEFECT_TEMPERATURE"
        
        # Doc review
        [ -n "$DOC_ENABLED" ] && DOCKER_ARGS="$DOCKER_ARGS -e DOC_ENABLED"
        [ -n "$DOC_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e DOC_MODEL"
        [ -n "$DOC_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DOC_MAX_ITERS"
        [ -n "$DOC_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DOC_MAX_CONTEXT_SIZE"
        [ -n "$DOC_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DOC_MAX_REASONING_TOKENS"
        [ -n "$DOC_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DOC_TEMPERATURE"
        
        # Smell review
        [ -n "$SMELL_ENABLED" ] && DOCKER_ARGS="$DOCKER_ARGS -e SMELL_ENABLED"
        [ -n "$SMELL_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e SMELL_MODEL"
        [ -n "$SMELL_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SMELL_MAX_ITERS"
        [ -n "$SMELL_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SMELL_MAX_CONTEXT_SIZE"
        [ -n "$SMELL_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SMELL_MAX_REASONING_TOKENS"
        [ -n "$SMELL_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SMELL_TEMPERATURE"
        
        # Supply chain
        [ -n "$SUPPLY_CHAIN_ENABLED" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUPPLY_CHAIN_ENABLED"
        [ -n "$SUPPLY_CHAIN_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUPPLY_CHAIN_MODEL"
        [ -n "$SUPPLY_CHAIN_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUPPLY_CHAIN_MAX_ITERS"
        [ -n "$SUPPLY_CHAIN_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUPPLY_CHAIN_MAX_CONTEXT_SIZE"
        [ -n "$SUPPLY_CHAIN_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUPPLY_CHAIN_MAX_REASONING_TOKENS"
        [ -n "$SUPPLY_CHAIN_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUPPLY_CHAIN_TEMPERATURE"
        [ -n "$SUPPLY_CHAIN_SCAN_UNCHANGED" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUPPLY_CHAIN_SCAN_UNCHANGED"
        
        # Deduplication
        [ -n "$DEDUPLICATION_ENABLED" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEDUPLICATION_ENABLED"
        [ -n "$DEDUPLICATION_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEDUPLICATION_MODEL"
        [ -n "$DEDUPLICATION_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEDUPLICATION_MAX_ITERS"
        [ -n "$DEDUPLICATION_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEDUPLICATION_MAX_CONTEXT_SIZE"
        [ -n "$DEDUPLICATION_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEDUPLICATION_MAX_REASONING_TOKENS"
        [ -n "$DEDUPLICATION_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e DEDUPLICATION_TEMPERATURE"
        
        # Summarization
        [ -n "$SUMMARIZATION_ENABLED" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUMMARIZATION_ENABLED"
        [ -n "$SUMMARIZATION_MODEL" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUMMARIZATION_MODEL"
        [ -n "$SUMMARIZATION_MAX_ITERS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUMMARIZATION_MAX_ITERS"
        [ -n "$SUMMARIZATION_MAX_CONTEXT_SIZE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUMMARIZATION_MAX_CONTEXT_SIZE"
        [ -n "$SUMMARIZATION_MAX_REASONING_TOKENS" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUMMARIZATION_MAX_REASONING_TOKENS"
        [ -n "$SUMMARIZATION_TEMPERATURE" ] && DOCKER_ARGS="$DOCKER_ARGS -e SUMMARIZATION_TEMPERATURE"
        
        # Other settings
        [ -n "$EXCLUDED_DIRECTORIES" ] && DOCKER_ARGS="$DOCKER_ARGS -e EXCLUDED_DIRECTORIES"
        
        # Build codespy command arguments
        CMD_ARGS="review ${{ steps.pr-url.outputs.url }}"
        CMD_ARGS="$CMD_ARGS --output ${{ inputs.output-format }}"
        
        # Add git comment flag
        if [ "${{ inputs.post-comment }}" = "true" ]; then
          CMD_ARGS="$CMD_ARGS --git-comment"
        else
          CMD_ARGS="$CMD_ARGS --no-git-comment"
        fi
        
        # Create output file for capturing review
        OUTPUT_FILE=$(mktemp)
        
        # Run codespy in Docker
        docker run $DOCKER_ARGS \
          ghcr.io/khezen/codespy:${{ inputs.codespy-version }} \
          $CMD_ARGS | tee "$OUTPUT_FILE"
        
        # Extract issue counts from output
        REVIEW_OUTPUT=$(cat "$OUTPUT_FILE")
        
        # Parse issue counts from markdown output
        # Matches lines like: - **Total Issues:** 5
        TOTAL_ISSUES=$(echo "$REVIEW_OUTPUT" | grep -oP '\*\*Total Issues:\*\*\s+\K\d+' || echo "0")
        CRITICAL_COUNT=$(echo "$REVIEW_OUTPUT" | grep -oP '\*\*Critical:\*\*\s+\K\d+' || echo "0")
        HIGH_COUNT=$(echo "$REVIEW_OUTPUT" | grep -oP '\*\*High:\*\*\s+\K\d+' || echo "0")
        
        # Set outputs
        echo "issues-count=$TOTAL_ISSUES" >> $GITHUB_OUTPUT
        echo "critical-count=$CRITICAL_COUNT" >> $GITHUB_OUTPUT
        echo "high-count=$HIGH_COUNT" >> $GITHUB_OUTPUT
        
        # Store full output (escape for multiline)
        {
          echo 'review-output<<EOF'
          cat "$OUTPUT_FILE"
          echo 'EOF'
        } >> $GITHUB_OUTPUT
        
        rm -f "$OUTPUT_FILE"

    - name: Check for critical issues
      if: inputs.fail-on-critical == 'true'
      shell: bash
      run: |
        if [ "${{ steps.review.outputs.critical-count }}" -gt "0" ]; then
          echo "::error::CodeSpy found ${{ steps.review.outputs.critical-count }} critical issue(s). Failing the action as requested."
          exit 1
        fi